{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Coffea with a DASK executor\n",
    "\n",
    "This will test Coffea to see if we can figure out how to use it with our code, but running with a local DASK executor.\n",
    "This doesn't make sense for this query - there is only one file, but it will test the pipeline!\n",
    "\n",
    "First are the includes from coffea. This is based on the [example written by Ben](https://github.com/CoffeaTeam/coffea/blob/master/binder/servicex/ATLAS/LocalExample.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from servicex import ServiceXDataset\n",
    "from coffea.processor.servicex import DataSource, Analysis\n",
    "from coffea.processor.servicex import DaskExecutor, LocalExecutor \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from coffea import hist, processor\n",
    "from IPython.display import display, update_display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And imports connected with running servicex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from func_adl import ObjectStream\n",
    "from func_adl_servicex import ServiceXSourceUpROOT\n",
    "from hist import Hist\n",
    "import mplhep as mpl\n",
    "import awkward as ak\n",
    "\n",
    "from utils import files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods copied to help us get all leptons from the source files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_event_cuts(source: ObjectStream) -> ObjectStream:\n",
    "    '''Event level cuts for the analysis. Keep from sending data that we aren't going to need at all in the end.\n",
    "    '''\n",
    "    return (source\n",
    "        .Where(lambda e: e.trigE or e.trigM))\n",
    "\n",
    "def good_leptons(source: ObjectStream) -> ObjectStream:\n",
    "    '''Select out all good leptons from each event. Return their pt, eta, phi, and E, and other\n",
    "    things needed downstream.\n",
    "\n",
    "    Because uproot doesn't tie toegher the objects, we can't do any cuts at this point.\n",
    "    '''\n",
    "    return source.Select(lambda e:\n",
    "        {\n",
    "            'lep_pt': e.lep_pt,\n",
    "            'lep_eta': e.lep_eta,\n",
    "            'lep_phi': e.lep_phi,\n",
    "            'lep_energy': e.lep_E,\n",
    "            'lep_charge': e.lep_charge,\n",
    "            'lep_ptcone30': e.lep_ptcone30,\n",
    "            'lep_etcone20': e.lep_etcone20,\n",
    "            'lep_type': e.lep_type,\n",
    "            'lep_trackd0pvunbiased': e.lep_trackd0pvunbiased,\n",
    "            'lep_tracksigd0pvunbiased': e.lep_tracksigd0pvunbiased,\n",
    "            'lep_z0': e.lep_z0,\n",
    "            'met_et': e.met_et\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the `func_adl` cuts to get the data. The dataset we use here doesn't matter, as long as it \"looks\" like all the datasets we are going to be processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ServiceXSourceUpROOT('cernopendata://dummy',  files['ggH125_ZZ4lep']['treename'], backend_name='uproot')\n",
    "ds.return_qastle = True\n",
    "leptons = good_leptons(apply_event_cuts(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis code that will apply the 4 lepton cuts and make the 4 lepton mass plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ATLAS_Higgs_4L(Analysis):\n",
    "    @staticmethod\n",
    "    def process(events):\n",
    "        import awkward as ak\n",
    "        from collections import defaultdict\n",
    "\n",
    "        sumw = defaultdict(float)\n",
    "        mass_hist = hist.Hist(\n",
    "            \"Events\",\n",
    "            hist.Cat(\"dataset\", \"Dataset\"),\n",
    "            hist.Bin(\"mass\", \"$Z_{ee}$ [GeV]\", 60, 60, 120),\n",
    "        )\n",
    "\n",
    "        dataset = events.metadata['dataset']\n",
    "        leptons = events.lep\n",
    "        met_et = events.met.et\n",
    "\n",
    "        # We need to look at 4 lepton events only.\n",
    "        cut = (ak.num(leptons) == 4)\n",
    "\n",
    "        # Form the invar mass, plot.\n",
    "        # diele = electrons[cut][:, 0] + electrons[cut][:, 1]\n",
    "        # diele.mass\n",
    "        dilepton = leptons[:,0] + leptons[:,1]\n",
    "        mass_4l = leptons.mass\n",
    "\n",
    "        # Fill the histogram\n",
    "        sumw[dataset] += len(events)\n",
    "        print(len(events))\n",
    "        mass_hist.fill(\n",
    "            dataset=dataset,\n",
    "            mass=ak.flatten(mass_4l),\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"sumw\": sumw,\n",
    "            \"mass\": mass_hist\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the data source that we will be running against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ds(name: str, query: ObjectStream):\n",
    "    '''Create a ServiceX Datasource for a particular ATLAS Open data file\n",
    "    '''\n",
    "    datasets = [ServiceXDataset(files[name]['files'], backend_name='uproot')]\n",
    "    return DataSource(query=query, metadata={'dataset': name}, datasets=datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2058/925832015.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Why do I need run_updates_stream, why not just await on execute (which fails with async gen can't).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Perhaps something from aiostream can help here?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mrun_updates_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalysis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2058/925832015.py\u001b[0m in \u001b[0;36mrun_updates_stream\u001b[0;34m(accumulator_stream)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcoffea_info\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maccumulator_stream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoffea_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/servicex/executor.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, analysis, datasource, title)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# There is an accumulate pattern in the aiostream lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mfinished_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstreamer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32min\u001b[0m \u001b[0masync_accumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstreamer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/accumulator.py\u001b[0m in \u001b[0;36masync_accumulate\u001b[0;34m(result_stream)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0masync_accumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult_stream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miadd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/aiostream/stream/advanced.py\u001b[0m in \u001b[0;36mbase_combine\u001b[0;34m(source, switch, ordered, task_limit)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m# Get result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;31m# End of stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/aiostream/stream/create.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miscoroutinefunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/servicex/executor.py\u001b[0m in \u001b[0;36minline_wait\u001b[0;34m(r)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minline_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;34m\"This could be inline, but python 3.6\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/servicex/local_executor.py\u001b[0m in \u001b[0;36m_async_analysis\u001b[0;34m(self, events_url, tree_name, data_type, meta_data, process_func)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     ):\n\u001b[0;32m---> 59\u001b[0;31m         return run_coffea_processor(\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mevents_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevents_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mtree_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtree_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/servicex/executor.py\u001b[0m in \u001b[0;36mrun_coffea_processor\u001b[0;34m(events_url, tree_name, proc, data_type, meta_data)\u001b[0m\n\u001b[1;32m    157\u001b[0m         ).events()\n\u001b[1;32m    158\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdata_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         events = NanoEventsFactory.from_parquet(\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevents_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mtreepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/nanoevents/factory.py\u001b[0m in \u001b[0;36mfrom_parquet\u001b[0;34m(cls, file, treepath, entry_start, entry_stop, runtime_cache, persistent_cache, schemaclass, metadata, parquet_options, rados_parquet_options, access_log)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mbase_form\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_base_form\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema_arrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         return cls._from_mapping(\n\u001b[0m\u001b[1;32m    237\u001b[0m             \u001b[0mmapping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mpartition_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/nanoevents/factory.py\u001b[0m in \u001b[0;36m_from_mapping\u001b[0;34m(cls, mapping, partition_key, base_form, runtime_cache, persistent_cache, schemaclass, metadata)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschemaclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid schemaclass type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschemaclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_form\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid schema type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/nanoevents/schemas/schema.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, base_form)\u001b[0m\n\u001b[1;32m     84\u001b[0m             )\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             record = _build_record_array(\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0mc_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mmapping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/nanoevents/schemas/schema.py\u001b[0m in \u001b[0;36m_build_record_array\u001b[0;34m(name, name_mapping, contents, record_name)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mcontents\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mcontents\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marray\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mare\u001b[0m \u001b[0mbuilding\u001b[0m \u001b[0minto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \"\"\"\n\u001b[0;32m---> 15\u001b[0;31m     items = {\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mv_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mv_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/nanoevents/schemas/schema.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \"\"\"\n\u001b[1;32m     15\u001b[0m     items = {\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mv_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mv_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     }\n",
      "\u001b[0;31mKeyError\u001b[0m: 'content'"
     ]
    }
   ],
   "source": [
    "analysis = ATLAS_Higgs_4L()\n",
    "\n",
    "executor = LocalExecutor()\n",
    "#executor = DaskExecutor(client_addr=\"tls://localhost:8786\")\n",
    "\n",
    "datasource = make_ds('ggH125_ZZ4lep', leptons)\n",
    "\n",
    "async def run_updates_stream(accumulator_stream):\n",
    "  global first\n",
    "\n",
    "  count = 0\n",
    "  async for coffea_info in accumulator_stream:\n",
    "    count += 1\n",
    "    print(count, coffea_info)\n",
    "  return coffea_info\n",
    "\n",
    "# Why do I need run_updates_stream, why not just await on execute (which fails with async gen can't).\n",
    "# Perhaps something from aiostream can help here?\n",
    "result = await run_updates_stream(executor.execute(analysis, datasource))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.plot1d(result['mass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "41c976a3581ca23dd5321440c1ba6aa61955a2d341cd42821c29af40225a7b53"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
